#!/usr/bin/env node

const {program} = require('commander')
const axios = require('axios');
const cheerio = require('cheerio')
const fs = require('fs')
const path = require('path')
const {URL} = require('url') 

//fetch html from a url 
//i can use axios for this
async function getHTML(url) {
    try {
        const response = await axios.get(url)
        const html = response.data
        return html
    } catch(error){
        console.error(`Error fetching URL: ${url}: ${error.message}`);
        throw error
    }
}

//uses cheerio to clean up the html
//nb there is also a cheerio .fromURL method i could prob use but I'll leave axios in for now 
function processHTML(html, baseUrl, linkLimit){
    const $ = cheerio.load(html)
    //pass the raw html to cheerio's load method
    //$ returns a cheerio object similar to an array of DOM elements

    //clean the html of parts we don't want 
    $('script').remove();
    $('.vector-header').remove();
    $('nav').remove();
    $('#p-lang-btn').remove();
    $('.infobox').remove();
    $('link[rel="stylesheet"]').remove();
    $('.ad, .advertisement').remove();
    $('form').remove();
    $('style').remove();
    $('meta').remove();

    //extract links up to limit
    const links = []
    const extractionLimit = linkLimit * 3
    //im adding this so that we can handle the issue of duplicate links but still return the requested amount 

    $('a[href]').each((index, element) => {
        if (links.length >= extractionLimit) return false;

         //get value of the href attribute of current element 
         let href = $(element).attr('href');
         if (href) {
             try {
                //handle protocol-relative Urls
                if (href.startsWith('//')){
                    href = 'https:' + href
                }
                //resolve relative Urls against base url 
                 const url = new URL(href, baseUrl);
                 links.push(url.toString());
             } catch (e) {
                 console.log(`Invalid URL: ${href}`);
             }
         }
    })
    links.sort()

    //return as an object with properties
    return {processedHTML: $.html(), extractedLinks: links} 
}

//save the output to a file using fs (filesystem) node library
function saveHTML(content, outputDir, filename){
    if (!fs.existsSync(outputDir)){
        fs.mkdirSync(outputDir, { recursive: true });
    }

    const filePath = path.join(outputDir, filename)

    fs.writeFileSync(filePath, content, 'utf-8')
    console.log(`Saved processed HTML to ${filePath}`)
}


const delay = (ms) => new Promise(resolve => setTimeout(resolve, ms));

//lets validate urls more stringently
function validateUrl(url){
    try{
        const parsedUrl = new URL(url)
        if (parsedUrl.protocol !== 'http:' && parsedUrl.protocol !== 'https:'){
            return {valid: false, message: `\nInvalid protocol: ${url.protocol}`}
        }
        if (!parsedUrl.hostname){
            return {valid: false, message: `\nMissing hostname`}
        }
        if (!/\.[a-z]{2,}$/i.test(parsedUrl.hostname)) {
            return {valid: false, message: `\nInvalid top-level domain`}
        }
        return {valid: true, message: ''}
    }
    catch(_){
        return {valid: false, message: 'URL parsing error'}
    } 
}

function fileExists(outputDir, fileName) {
    return fs.existsSync(path.join(outputDir, fileName));
}


//calls all the functions to scrape a site, then recursively calls itself to traverse links
async function scrape(url, baseUrl, linkLimit, depth, outputDir, visited = new Set()){

    //if depth is 0, we don't want to traverse anymore, return
    if (visited.has(url)){
        console.log(`skipping url duplicate: ${url}`)
        return;

    } else if (depth === 0){
        console.log(`reached max depth, stopping at url: ${url}`)
        return

    } 
    console.log(`Attempting to scrape URL at depth ${depth}: ${url}`);

    //add the url so we can track we don't visit the same one multiple times
    visited.add(url)

    
    try {
        //another check for valid url format 
        //get the html
        const html = await getHTML(url)

        if (html === null) {
            console.log(`Skipping ${url} due to 404 error`)
            return
        } 

        console.log(`Successfully fetched HTML for ${url}`);

        //clean up html and get links
        const {processedHTML, extractedLinks} = processHTML(html, baseUrl, linkLimit)

        console.log(`Processed HTML`, depth > 1? `& extracted ${extractedLinks.length} links`: ``);

        //save html to output file based on sanitized url
        const fileName = `${depth}_${url.replace(/[^a-z0-9]/gi, "_")}.html`;
        if (processedHTML){
            if (fileExists(outputDir, fileName)) {
                //might remove this, without other changes it will cause everything downstream to be skipped too 
                console.log(`File for ${url} already exists, skipping.`);
                return;
            } else {
                saveHTML(processedHTML, outputDir, fileName)

            }
        }
        
        if (depth > 1){
            //going to add capacity to add more links up to limit in case of skipped duplicates
            let uniqueLinksProcessed = 0
            for(let i = 0; i < extractedLinks.length && uniqueLinksProcessed < linkLimit; i++){
                const link = extractedLinks[i];
                if (!visited.has(link)) {
                    await delay(500);
                    await scrape(link, baseUrl, linkLimit, depth - 1, outputDir, visited);
                    uniqueLinksProcessed++;
                }
            }
        }
    } catch (error){
        console.error(`Error scraping ${url}: ${error.message}`);
        throw error;

    }
}

program
    .version('1.0.0')
    .description('A simple HTML scraper CLI tool')
    .requiredOption('-u, --url <url>', 'URL to scrape')
    .option('-d, --depth <number>', 'Recursive depth to scrape', '1')
    .option('-l, --limit <number>', 'Limit of links to scrape per page', '0')
    .option('-o, --output <directory>', 'Output directory', './output')
    .parse(process.argv)


//this is going to pull out the users inputs
const options = program.opts()

//passes parameters into recursive scrape function 
async function main() {
    const url = options.url;
    const linkLimit = parseInt(options.limit)
    const depth = parseInt(options.depth)
    const outputDir = options.output

    const validateResult = validateUrl(url)
    if (!validateResult.valid){
        console.error(`Invalid url: ${url}`, validateResult.message? validateResult.message: '')
        process.exit(1)
    }

    //create url object with js URL interface to get the base url for standardization of internal links
    const baseUrl = new URL(url).origin;
    

    try{
        await scrape(url, baseUrl, linkLimit, depth, outputDir);
        console.log("Scraping completed successfully, check output folder for your new HTML files! ᕙ(‾̀◡‾́)ᕗ")
    } catch(error){
        //ok, improve this later - currently, any error causes this message to be logged, even if some scraping succeeded
        console.error("An error was returned 乁(ツ゚)ㄏ : ", error.message)
        process.exit(1);
    }
    
}

main().catch(error => {
    console.error('An error occured: ', error)
    process.exit(1);
});